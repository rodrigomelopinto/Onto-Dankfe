{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodrirocki/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/covid/America/\n",
      "data/covid/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'utils')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_charts as ds\n",
    "from ds_charts import HEIGHT\n",
    "import numpy as np\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import seaborn as sns\n",
    "import json\n",
    "import jstyleson\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler ,LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from rdflib import Graph, Namespace, URIRef, Literal\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# sys.stdout = open('Results/dankfe_1_timings.txt', 'a')\n",
    "\n",
    "\n",
    "base_name = \"covid\"\n",
    "data_folder = f'data/{base_name}/America/'\n",
    "model_folder = f'data/{base_name}/'\n",
    "print(data_folder)\n",
    "print(model_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "start_time = timeit.default_timer()\n",
    "if base_name == \"covid\":\n",
    "    data = pd.read_csv(f'{data_folder}{base_name}_base.csv',parse_dates=['current_date','first_date'], infer_datetime_format=True)\n",
    "else:\n",
    "    data = pd.read_csv(f'{data_folder}{base_name}_base.csv',parse_dates=['current_date'], infer_datetime_format=True)\n",
    "# print('read csv')\n",
    "read_time = timeit.default_timer() - start_time\n",
    "\n",
    "# Getting ER Model of case study data\n",
    "f = open(f'{model_folder}{base_name}_model.json')\n",
    "model = jstyleson.load(f)\n",
    "\n",
    "#Getting Ontology of case study data\n",
    "ontology = Graph()\n",
    "ontology.parse(f'{model_folder}{base_name}_model.rdf')\n",
    "\n",
    "# Getting options JSON\n",
    "f = open(f'{model_folder}options.json')\n",
    "options = jstyleson.load(f)\n",
    "\n",
    "# Getting variable template JSON\n",
    "f = open(f'utils/var_template.json')\n",
    "template = jstyleson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'high_risk_2w'\n",
    "\n",
    "# target_count = data[target].value_counts()\n",
    "# positive_class = target_count.idxmin()\n",
    "# negative_class = target_count.idxmax()\n",
    "# print('Minority class=', positive_class, ':', target_count[positive_class])\n",
    "# print('Majority class=', negative_class, ':', target_count[negative_class])\n",
    "# print('Proportion:', round(target_count[positive_class] / target_count[negative_class], 2), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_name_list = [entity['name'] for entity in model['entities']]\n",
    "# # Raise Error if entities do not match columns:\n",
    "# if collections.Counter(entity_name_list) != collections.Counter(data.columns.to_list()):\n",
    "#     raise ValueError(\"Entities and Columns do not have the same size.\")\n",
    "\n",
    "# Remove all non-observed columns\n",
    "#not_observed = [entity['name'] for entity in model['entities'] if not entity['observed']]\n",
    "\n",
    "# Get names of columns to create\n",
    "#created_vars = [relation['output'] for relation in model['relations']]\n",
    "\n",
    "# Convert date entities to timestamp\n",
    "#date_columns = [entity['name'] for entity in model['entities'] if entity['type'] == 'datetime']\n",
    "# for col in date_columns:\n",
    "#     data[col] = pd.to_datetime(data[col], format = \"%Y-%m-%d\")\n",
    "# data['current_date'] = pd.to_datetime(data['current_date'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "# data['current_date'] = pd.to_datetime(data['current_date'], format = \"%d/%m/%Y\")\n",
    "# data['first_date'] = pd.to_datetime(data['first_date'], format = \"%d/%m/%Y\")\n",
    "#data_observed = data.drop(columns = not_observed)\n",
    "# data_observed = data_observed.sample(frac = 0.1, random_state = 0).reset_index(drop = True)\n",
    "# data_observed = data_observed[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)\n",
    "# ds.get_variable_types(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "def data_preparation(dataset,options,target):\n",
    "    if dataset[target].isna().sum() != 0:\n",
    "        # print(\"Target MVs: dropped rows\")\n",
    "        dataset = dataset.dropna(subset=[target]).reset_index(drop = True)\n",
    "    dataset = encodeLabels(dataset)\n",
    "    if options['checkMissingValues'] != 'none':\n",
    "        dataset = checkMissingValues(dataset,options['checkMissingValues'])\n",
    "    return dataset\n",
    "    # generate Variables\n",
    "    # if options['checkScaling'] != 'none':\n",
    "    #     dataset = checkScaling(dataset,options['checkScaling'])\n",
    "    # # divide into train and test\n",
    "    # data_train, data_test = train_test_split(dataset,train_size=0.7,test_size=0.3,stratify=dataset[target],random_state=1)\n",
    "    # if options['checkBalancing']:\n",
    "    #     data_train = checkBalancing(data_train)\n",
    "    # return data_train,data_test\n",
    "\n",
    "def data_preparation_2(dataset,options,target):\n",
    "    if options['checkScaling'] != 'none':\n",
    "        dataset = checkScaling(dataset,options['checkScaling'])\n",
    "    data_train, data_test = train_test_split(dataset,train_size=0.7,test_size=0.3,stratify=dataset[target],random_state=1)\n",
    "    if options['checkBalancing']:\n",
    "        data_train = checkBalancing(data_train)\n",
    "    data_train = data_train.reset_index(drop=True)\n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    return data_train,data_test\n",
    "\n",
    "def encodeLabels(dataset):\n",
    "    le = LabelEncoder()\n",
    "    bool_df = dataset.select_dtypes(include='bool')\n",
    "    if len(bool_df.columns) != 0:\n",
    "        for col in bool_df:\n",
    "            dataset[col] = le.fit_transform(dataset[col])\n",
    "    return dataset\n",
    "\n",
    "def checkScaling(dataset,method):\n",
    "    # print(f\"Checking scaling: method {method}\")\n",
    "    numeric_vars, symbolic_vars, binary_vars, date_vars = ds.get_variable_types(dataset).values()\n",
    "    \n",
    "    if method == 'zscore':\n",
    "        transf = StandardScaler(with_mean=True, with_std=True, copy=True).fit(dataset[numeric_vars])\n",
    "    if method == 'minmax':\n",
    "        transf = MinMaxScaler(feature_range=(0,1), copy=True).fit(dataset[numeric_vars])\n",
    "    tmp = pd.DataFrame(transf.transform(dataset[numeric_vars]), index = dataset.index, columns = numeric_vars)\n",
    "    data_scaled = pd.concat([tmp,dataset[symbolic_vars],dataset[binary_vars],dataset[date_vars]], axis = 1)\n",
    "    return data_scaled\n",
    "\n",
    "def checkBalancing(dataset):\n",
    "    target_count = data[target].value_counts()\n",
    "    proportion = target_count[target_count.idxmin()] / target_count[target_count.idxmax()]\n",
    "    data_bal = dataset.copy(deep=True)\n",
    "    df_min = data_bal[data_bal[target] == target_count.idxmin()]\n",
    "    df_max = data_bal[data_bal[target] == target_count.idxmax()]\n",
    "    if proportion <= 0.66:\n",
    "        if target_count[target_count.idxmin()] >= 25000 and target_count[target_count.idxmax()] >= 25000:\n",
    "            print(\"Checking balancing: Undersampling both classes - both over 25000\")\n",
    "            df_min = df_min.sample(n=25000, replace = False, random_state = 1)\n",
    "            df_max = df_max.sample(n=25000, replace = False, random_state = 1)\n",
    "        elif target_count[target_count.idxmin()] < 25000 and target_count[target_count.idxmax()] >= 25000:\n",
    "            print(\"Checking balancing: Oversampling min class and undersampling max class - one over 25000\")\n",
    "            df_min = df_min.sample(n=25000, replace = True, random_state = 1)\n",
    "            df_max = df_max.sample(n=25000, replace = False, random_state = 1)\n",
    "        else:\n",
    "            print(\"Checking balancing: Oversampling min class - none over 25000\")\n",
    "            df_min = df_min.sample(n=len(df_max), replace = True, random_state = 1)\n",
    "        data_balanced = pd.concat([df_min,df_max],axis = 0)\n",
    "        return data_balanced\n",
    "    else:\n",
    "        print(\"Checking balancing: No balancing required.\")\n",
    "        return dataset\n",
    "\n",
    "def checkMissingValues(dataset,method):\n",
    "    # print(f\"Checking MVs: method {method}\")\n",
    "    numeric_vars, binary_vars, date_vars, symbolic_vars = ds.get_variable_types(dataset).values()\n",
    "    if method == 'auto':\n",
    "        tmp_nr, tmp_sb, tmp_bool = None, None, None\n",
    "        if len(numeric_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='median', missing_values=np.nan, copy=True)\n",
    "            tmp_nr = pd.DataFrame(imp.fit_transform(dataset[numeric_vars]), index =dataset.index ,columns=numeric_vars)\n",
    "        if len(symbolic_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='most_frequent', missing_values=np.nan, copy=True)\n",
    "            tmp_sb = pd.DataFrame(imp.fit_transform(dataset[symbolic_vars]), index=dataset.index ,columns=symbolic_vars)\n",
    "        if len(binary_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='most_frequent', missing_values=np.nan, copy=True)\n",
    "            tmp_bool = pd.DataFrame(imp.fit_transform(dataset[binary_vars]), index=dataset.index ,columns=binary_vars)\n",
    "    data_mv = pd.concat([tmp_nr, tmp_sb, tmp_bool,dataset[date_vars]], axis=1)\n",
    "    return data_mv\n",
    "\n",
    "# data_prepared_train, data_prepared_test = data_preparation(data,options,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic variable generation\n",
    "\n",
    "def generateAutoVariables(dataset,options,template,model):\n",
    "    if options['generateDates']:\n",
    "        model = generateDates(dataset,template['dates'])\n",
    "    if len(options['generateFiveSummary']) != 0:\n",
    "        model = generateFiveSummary(dataset,template['five_summary'], options['groupby'], options['generateFiveSummary'])\n",
    "    return model\n",
    "\n",
    "def generateDates(dataset,template):\n",
    "    date_vars = ds.get_variable_types(dataset)['Date']\n",
    "    new_datevars = []\n",
    "    template_tmp = copy.deepcopy(template)\n",
    "    for date_col in date_vars:\n",
    "        template_tmp = copy.deepcopy(template)\n",
    "        for new_var in template_tmp:\n",
    "            new_var['output'] = f\"{date_col}_{new_var['output']}\"\n",
    "            new_var['inputs'].append(date_col)\n",
    "            new_datevars.append(new_var)\n",
    "    # print(f\"Adding {len(new_datevars)} new variables\")\n",
    "    model['relations'] =  model['relations'] + new_datevars\n",
    "    return model\n",
    "    \n",
    "def generateFiveSummary(dataset,template,groupby,num_vars):\n",
    "    # num_vars = ds.get_variable_types(dataset)['Numeric']\n",
    "    new_numvars = []\n",
    "    template_tmp = copy.deepcopy(template)\n",
    "    if len(groupby) != 0:\n",
    "        for group in groupby:\n",
    "            for num_col in num_vars:\n",
    "                template_tmp = copy.deepcopy(template)\n",
    "                for new_var in template_tmp:\n",
    "                    new_var['output'] = f\"{num_col}_{new_var['output']}_per_{group}\"\n",
    "                    new_var['inputs'].append(num_col)\n",
    "                    new_var['groupby'] = group\n",
    "                    new_numvars.append(new_var)\n",
    "    # print(f\"Adding {len(new_numvars)} new variables\")\n",
    "    model['relations'] = model['relations'] + new_numvars\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Generation\n",
    "holidays = pd.read_csv('holidays_europe.csv')\n",
    "holidays['date'] =  pd.to_datetime(holidays['date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "day_periods = [(1, (datetime.datetime(2000,1,1,0,0,0),  datetime.datetime(2000,1,1,2,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,3,0,0),  datetime.datetime(2000,1,1,5,59,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,6,0,0),  datetime.datetime(2000,1,1,8,59,59))),\n",
    "           (4, (datetime.datetime(2000,1,1,9,0,0),  datetime.datetime(2000,1,1,11,59,59))),\n",
    "           (5, (datetime.datetime(2000,1,1,12,0,0),  datetime.datetime(2000,1,1,14,59,59))),\n",
    "           (6, (datetime.datetime(2000,1,1,15,0,0),  datetime.datetime(2000,1,1,17,59,59))),\n",
    "           (7, (datetime.datetime(2000,1,1,18,0,0),  datetime.datetime(2000,1,1,20,59,59))),\n",
    "           (8, (datetime.datetime(2000,1,1,21,0,0),  datetime.datetime(2000,1,1,23,59,59)))]\n",
    "\n",
    "energy_prices = [(1, (datetime.datetime(2000,1,1,22,0,0),  datetime.datetime(2000,1,1,23,59,59))),\n",
    "           (1, (datetime.datetime(2000,1,1,0,0,0),  datetime.datetime(2000,1,1,7,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,8,0,0),  datetime.datetime(2000,1,1,8,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,10,30,0),  datetime.datetime(2000,1,1,17,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,20,30,0),  datetime.datetime(2000,1,1,21,59,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,9,0,0),  datetime.datetime(2000,1,1,10,29,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,18,0,0),  datetime.datetime(2000,1,1,20,29,59)))]\n",
    "\n",
    "seasons = [(1, (datetime.date(2000,  1,  1),  datetime.date(2000,  3, 20))),\n",
    "           (2, (datetime.date(2000,  3, 21),  datetime.date(2000,  6, 20))),\n",
    "           (3, (datetime.date(2000,  6, 21),  datetime.date(2000,  9, 22))),\n",
    "           (4, (datetime.date(2000,  9, 23),  datetime.date(2000, 12, 20))),\n",
    "           (1, (datetime.date(2000, 12, 21),  datetime.date(2000, 12, 31)))]\n",
    "\n",
    "center_baltimore = (39.30746849825375, -76.61560625253648)\n",
    "\n",
    "energy_types = [\"coal\",\"nuclear\",\"ccgt\",\"wind\",\"pumped\",\"hydro\",\"biomass\",\"oil\",\"solar\",\"ocgt\"]\n",
    "\n",
    "renewable_types = [\"wind\",\"pumped\",\"hydro\",\"biomass\",\"solar\"]\n",
    "\n",
    "crime_type = [\n",
    "    (1, 'LARCENY'), (2,'LARCENY FROM AUTO'), (3,'AUTO THEFT'), (11,'ROBBERY - STREET'), (12,'ROBBERY - COMMERCIAL'), (13,'ROBBERY - CARJACKING'),(14,'ROBBERY - RESIDENCE'),(15,'BURGLARY'),(21,'COMMON ASSAULT'),(31,'ASSAULT BY THREAT'),(32,'AGG. ASSAULT'),(41,'ARSON'),(51,'SHOOTING'),(52,'RAPE'),(61,'HOMICIDE')\n",
    "]\n",
    "weapon_type = [\n",
    "    (0, 'NONE'), (1,'HANDS'), (2,'OTHER'), (3,'KNIFE'), (4,'FIREARM')\n",
    "]\n",
    "\n",
    "# Kept data for operations\n",
    "temp_data = None\n",
    "\n",
    "def determine_season(day, month):\n",
    "    if (month == 3 and day >= 20) or (month == 4 or month == 5) or (month == 6 and day < 21):\n",
    "        return \"Spring\"\n",
    "    elif (month == 6 and day >= 21) or (month == 7 or month == 8) or (month == 9 and day < 23):\n",
    "        return \"Summer\"\n",
    "    elif (month == 9 and day >= 23) or (month == 10 or month == 11) or (month == 12 and day < 21):\n",
    "        return \"Autumn\"\n",
    "    else:\n",
    "        return \"Winter\"\n",
    "\n",
    "def process_numeric_variables(data):\n",
    "    numeric_vars = ds.get_variable_types(data)['Numeric']\n",
    "    for numeric_var in numeric_vars:\n",
    "        values = data[numeric_var].astype(float)\n",
    "        min = values.min()\n",
    "        max = values.max()\n",
    "        median = np.median(values)\n",
    "        std = np.std(values)\n",
    "        mean = np.mean(values)\n",
    "\n",
    "        data[f\"{numeric_var}_min\"] = min\n",
    "        data[f\"{numeric_var}_max\"] = max\n",
    "        data[f\"{numeric_var}_median\"] = median\n",
    "        data[f\"{numeric_var}_std\"] = std\n",
    "        data[f\"{numeric_var}_mean\"] = mean\n",
    "    return data\n",
    "\n",
    "def process_date_variables(data, row, idx):\n",
    "    date_vars = ds.get_variable_types(data)['Date']\n",
    "    for date_var in date_vars:\n",
    "        date_value = row[date_var]\n",
    "        date_obj = date_value.strftime('%Y-%m-%d')\n",
    "        date_obj = datetime.datetime.strptime(date_obj, \"%Y-%m-%d\")\n",
    "        day = date_obj.day\n",
    "        month = date_obj.month\n",
    "        year = date_obj.year\n",
    "        season =  determine_season(day, month)\n",
    "\n",
    "        data.at[idx, f\"{date_var}_day\"] = day\n",
    "        data.at[idx, f\"{date_var}_month\"] = month\n",
    "        data.at[idx, f\"{date_var}_year\"] = year\n",
    "        data.at[idx, f\"{date_var}_season\"] = season\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def process_binary_variables(data):\n",
    "    binary_vars = ds.get_variable_types(data)['Binary']\n",
    "    for binary_var in binary_vars:\n",
    "        unique_values = data[binary_var].unique()\n",
    "\n",
    "        if len(unique_values) == 2:\n",
    "            data[binary_var] = data[binary_var].map({unique_values[0]: 0, unique_values[1]: 1})\n",
    "        else:\n",
    "            print(f\"Warning: Binary variable '{binary_var}' does not have exactly two unique values.\")\n",
    "    return data\n",
    "\n",
    "def process_symbolic_variable(data, encoding_type):\n",
    "    symbolic_vars = ds.get_variable_types(data)['Symbolic']\n",
    "    if encoding_type == 'one_hot':\n",
    "        data = pd.get_dummies(data, columns=symbolic_vars)\n",
    "    elif encoding_type == 'label':\n",
    "        for var in symbolic_vars:\n",
    "            data[var] = data[var].astype('category').cat.codes\n",
    "    else:\n",
    "        print('Invalid encoding type')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# DANKFE 5\n",
    "def dankfe_5(ontology,data):\n",
    "    data = process_numeric_variables(data)\n",
    "    data = process_binary_variables(data)\n",
    "    for index, row in data.iterrows():\n",
    "        data = process_date_variables(data,row, index)\n",
    "    data = process_symbolic_variable(data, 'one_hot')\n",
    "    start_time = timeit.default_timer()\n",
    "    data = decompositions(ontology,data)\n",
    "    print('Decomp op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = mapping(ontology,data)\n",
    "    print('Mapping op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = algebraic(ontology,data)\n",
    "    print('Algebric op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = aggregations(ontology,data)\n",
    "    print('Aggregation op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = compositions(ontology,data)\n",
    "    print('Composition op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DANKFE 2\n",
    "def dankfe_2(model,loop_dataset,edit_dataset):\n",
    "    global temp_data\n",
    "    relations_queue = model['relations']\n",
    "\n",
    "    while len(relations_queue) != 0:\n",
    "        current_relation = relations_queue[0]\n",
    "        print(current_relation['output'])\n",
    "        newvar_name = current_relation['output']\n",
    "        inputs = current_relation['inputs']\n",
    "        groupby = current_relation['groupby']\n",
    "        if set(inputs).issubset(loop_dataset.columns):         # if inputs already exist in the dataset\n",
    "            start_time = timeit.default_timer()\n",
    "            # if len(current_relation['constraint']) == 0:\n",
    "            #     constraint = \"index == index\"\n",
    "            # else:\n",
    "            constraint = current_relation['constraint']\n",
    "            if len(current_relation['constraint']) != 0:\n",
    "                constraint = \"row.\" + constraint\n",
    "            else:\n",
    "                constraint = \"True\"\n",
    "            if len(groupby) == 0: #LAMBDA: no row dependence\n",
    "                for index, op in enumerate(current_relation['operations']):\n",
    "                    if index == 0:\n",
    "                        edit_dataset[newvar_name] = edit_dataset.apply(lambda row: get_operation(op,*zip(inputs,row[inputs])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "                    else:\n",
    "                        edit_dataset[newvar_name] = edit_dataset.apply(lambda row: get_operation(op,zip(newvar_name,row[newvar_name])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "            else:\n",
    "                needed_rows = current_relation['needsRows']\n",
    "                for i in range(len(loop_dataset)):\n",
    "                    # counter += 1\n",
    "                    row = loop_dataset.loc[i]  # we pass the list to get a DataFrame instead of Series\n",
    "                    if len(groupby) != 0:\n",
    "                        if needed_rows == 'all':\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]]\n",
    "                        elif needed_rows < 0:\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]].loc[i + needed_rows+1:i]\n",
    "                        else:\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]].loc[i:i + needed_rows-1] # get necessary rows to temp_data\n",
    "                    # else:\n",
    "                    #     if needed_rows < 0: \n",
    "                    #         temp_data = dataset.loc[i+ needed_rows:i]\n",
    "                    #     else:    \n",
    "                    #         temp_data = dataset.loc[i:i + needed_rows]\n",
    "                    if pd.eval(constraint, target = row) == False:\n",
    "                        edit_dataset.loc[i,newvar_name] = np.nan\n",
    "                        continue\n",
    "                    else:\n",
    "                        for opIndex, op in enumerate(current_relation['operations']):\n",
    "                            input_values =  [row.loc[inp] for inp in inputs]\n",
    "                            if opIndex == 0:\n",
    "                                edit_dataset.loc[i,newvar_name] = get_operation(op,*zip(inputs,input_values))\n",
    "                            else:\n",
    "                                edit_dataset.loc[i,newvar_name] = get_operation(op,zip(newvar_name,edit_dataset.loc[i,newvar_name]))\n",
    "            relations_queue = relations_queue[1:]\n",
    "            loop_dataset = edit_dataset.copy(deep=True)\n",
    "            print(timeit.default_timer() - start_time)\n",
    "        else:\n",
    "            # send to the bottom of the queue\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "\n",
    "\n",
    "# DANKFE 1\n",
    "def dankfe_1(model,dataset):\n",
    "    relations_queue = model['relations']\n",
    "\n",
    "    while len(relations_queue) != 0:\n",
    "        current_relation = relations_queue[0]\n",
    "        if len(current_relation['groupby']) != 0:\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "            continue\n",
    "        print(current_relation['output'])\n",
    "        newvar_name = current_relation['output']\n",
    "        inputs = current_relation['inputs']\n",
    "        if set(inputs).issubset(dataset.columns):\n",
    "            start_time = timeit.default_timer()\n",
    "            # if inputs already exist in the dataset\n",
    "            constraint = current_relation['constraint']\n",
    "            if len(current_relation['constraint']) != 0:\n",
    "                constraint = \"row.\" + constraint\n",
    "            else:\n",
    "                constraint = \"True\"\n",
    "            for index, op in enumerate(current_relation['operations']):\n",
    "                if index == 0:\n",
    "                    dataset[newvar_name] = dataset.apply(lambda row: get_operation(op,*zip(inputs,row[inputs])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "                else:\n",
    "                    # dataset[newvar_name] = dataset.apply(lambda row: print(row[newvar_name]), axis = 1)\n",
    "                    dataset[newvar_name] = dataset.apply(lambda row: get_operation(op,(newvar_name,row[newvar_name])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "            relations_queue = relations_queue[1:]\n",
    "            print(timeit.default_timer() - start_time)\n",
    "        else:\n",
    "            # send to the bottom of the queue\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DANKFE 4\n",
    "def dankfe_4(ontology,data):\n",
    "    start_time = timeit.default_timer()\n",
    "    data = decompositions(ontology,data)\n",
    "    print('Decomp op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = mapping(ontology,data)\n",
    "    print('Mapping op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = algebraic(ontology,data)\n",
    "    print('Algebric op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = aggregations(ontology,data)\n",
    "    print('Aggregation op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    start_time = timeit.default_timer()\n",
    "    data = compositions(ontology,data)\n",
    "    print('Composition op time:')\n",
    "    print(timeit.default_timer() - start_time)\n",
    "    return data\n",
    "\n",
    "\n",
    "def decompositions(ontology, data):\n",
    "    vars = extract_variables_decomp(ontology)\n",
    "    new_variables = {}\n",
    "    \n",
    "    # Read the CSV file\n",
    "    for idx ,row in data.iterrows():\n",
    "        for var in vars:\n",
    "            res = apply_decomp_rules(row[vars[var]], var)\n",
    "            new_variables = {res[0] : res[1]}\n",
    "            # Update the row with new variables\n",
    "            for key, value in new_variables.items():\n",
    "                data.at[idx, key] = value\n",
    "    \n",
    "    return data\n",
    "\n",
    "def mapping(ontology, data):\n",
    "    vars = extract_variables_mapping(ontology)\n",
    "    new_variables = {}\n",
    "    \n",
    "    # Read the CSV file\n",
    "    for idx, row in data.iterrows():\n",
    "        for var in vars:\n",
    "        # Apply rules to the specified variable in the row\n",
    "            res = apply_mapping_rules(row[vars[var]], var)\n",
    "            new_variables = {res[0] : res[1]}\n",
    "        # Update the row with new variables\n",
    "            for key, value in new_variables.items():\n",
    "                data.at[idx, key] = value\n",
    "    \n",
    "    return data\n",
    "\n",
    "def aggregations(ontology, data):\n",
    "    vars = extract_variables_aggregation(ontology)\n",
    "    new_variables = {}\n",
    "\n",
    "    for var in vars:\n",
    "        if vars[var][0] == 'sum':\n",
    "            aggregated_values = {}\n",
    "\n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "\n",
    "                if group_value not in aggregated_values:\n",
    "                    aggregated_values[group_value] = {vars[var][1] : 0}\n",
    "\n",
    "                aggregated_values[group_value][vars[var][1]] += int(row[vars[var][1]])\n",
    "            \n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "                new_variables[var] = aggregated_values[group_value][vars[var][1]]\n",
    "\n",
    "                for key, value in new_variables.items():\n",
    "                    data.at[idx, key] = value\n",
    "\n",
    "        if vars[var][0] == 'avg':\n",
    "            aggregated_values = {}\n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "\n",
    "                if group_value not in aggregated_values:\n",
    "                    aggregated_values[group_value] = {'count': 0, 'total': 0}\n",
    "\n",
    "                aggregated_values[group_value]['count'] += 1\n",
    "                aggregated_values[group_value]['total'] += int(row[vars[var][1]])\n",
    "            \n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "                new_variables[var] = aggregated_values[group_value]['total'] / aggregated_values[group_value]['count']\n",
    "\n",
    "                for key, value in new_variables.items():\n",
    "                    data.at[idx, key] = value\n",
    "        \n",
    "        if vars[var][0] == 'max':\n",
    "            aggregated_values = {}\n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "\n",
    "                if group_value not in aggregated_values:\n",
    "                    aggregated_values[group_value] = {'max': 0}\n",
    "\n",
    "                if aggregated_values[group_value]['max'] <= row[vars[var][1]]:\n",
    "                    aggregated_values[group_value]['max'] = row[vars[var][1]]\n",
    "            \n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "                new_variables[var] = aggregated_values[group_value]['max']\n",
    "\n",
    "                for key, value in new_variables.items():\n",
    "                    data.at[idx, key] = value\n",
    "        \n",
    "        if vars[var][0] == 'min':\n",
    "            aggregated_values = {}\n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "\n",
    "                if group_value not in aggregated_values:\n",
    "                    aggregated_values[group_value] = {'min': row[vars][var][1]}\n",
    "\n",
    "                if aggregated_values[group_value]['min'] >= row[vars[var][1]]:\n",
    "                    aggregated_values[group_value]['min'] = row[vars[var][1]]\n",
    "            \n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "                new_variables[var] = aggregated_values[group_value]['min']\n",
    "\n",
    "                for key, value in new_variables.items():\n",
    "                    data.at[idx, key] = value\n",
    "\n",
    "        if vars[var][0] == 'std':\n",
    "            aggregated_values = {}\n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "\n",
    "                if group_value not in aggregated_values:\n",
    "                    aggregated_values[group_value] = {'count': 0, 'total': 0, 'squared_total' : 0}\n",
    "\n",
    "                aggregated_values[group_value]['count'] += 1\n",
    "                aggregated_values[group_value]['total'] += int(row[vars[var][1]])\n",
    "                aggregated_values[group_value]['squared_total'] += int(row[vars[var][1]]) ** 2\n",
    "            \n",
    "            for idx, row in data.iterrows():\n",
    "                group_value = row[vars[var][2]]\n",
    "                mean = aggregated_values[group_value]['total'] / aggregated_values[group_value]['count']\n",
    "                squared_mean = aggregated_values[group_value]['squared_total'] / aggregated_values[group_value]['count']\n",
    "                new_variables[var] = np.sqrt(squared_mean - mean ** 2)\n",
    "\n",
    "                for key, value in new_variables.items():\n",
    "                    data.at[idx, key] = value\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def compositions(ontology, data):\n",
    "    swrl_rules = {}\n",
    "\n",
    "    body_atoms = []\n",
    "\n",
    "    for s, p, o in ontology.triples((None, URIRef('http://www.w3.org/2003/11/swrl#body'), None)):\n",
    "        atom_list = [o]\n",
    "        while True:\n",
    "            for s, p, o in ontology.triples((atom_list[-1], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first'), None)):\n",
    "                atom_list.append(o)\n",
    "            rest = next(ontology.objects(atom_list[-2], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest')))\n",
    "            if rest == URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#nil'):\n",
    "                break\n",
    "            atom_list.append(rest)\n",
    "\n",
    "        atom_info = {}\n",
    "        arguments = {}\n",
    "        list_builtin = []\n",
    "        variables_connection = {}\n",
    "        atom_info['argument_variables'] = arguments\n",
    "        for atom in atom_list:\n",
    "            if (atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'), None) in ontology:\n",
    "                variable = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'))).split('/')[-1]\n",
    "                argument = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#argument1'))).split('/')[-1]\n",
    "                variables_connection[argument] = variable\n",
    "                atom_info['argument_variables'][variable] = variable\n",
    "            elif (atom, URIRef('http://www.w3.org/2003/11/swrl#builtin'), None) in ontology:\n",
    "                builtin = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#builtin'))).split('/')[-1]\n",
    "                arguments = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#arguments'))).split('/')[-1]\n",
    "                arguments = list(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#arguments')))\n",
    "                for arg in arguments:\n",
    "                    list_items = {}\n",
    "                    list_items[builtin.split('#')[-1]] = []\n",
    "                    for item in ontology.items(arg):\n",
    "                        # If the list item is a resource, print its URI\n",
    "                        if isinstance(item, URIRef):\n",
    "                            if str(item).split('/')[-1] in variables_connection:\n",
    "                                list_items[builtin.split('#')[-1]].append(variables_connection[str(item).split('/')[-1]])\n",
    "                            else:\n",
    "                                list_items[builtin.split('#')[-1]].append(str(item).split('/')[-1])\n",
    "                        # If the list item is a literal, print its value\n",
    "                        elif isinstance(item, Literal):\n",
    "                            list_items[builtin.split('#')[-1]].append(str(item))\n",
    "                    list_builtin.append(list_items)\n",
    "                atom_info['builtins'] = list_builtin\n",
    "        body_atoms.append(atom_info)\n",
    "\n",
    "\n",
    "    head_atoms = []\n",
    "\n",
    "    for s, p, o in ontology.triples((None, URIRef('http://www.w3.org/2003/11/swrl#head'), None)):\n",
    "        atom_list = [o]\n",
    "        while True:\n",
    "            for s, p, o in ontology.triples((atom_list[-1], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first'), None)):\n",
    "                atom_list.append(o)\n",
    "            rest = next(ontology.objects(atom_list[-2], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest')))\n",
    "            if rest == URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#nil'):\n",
    "                break\n",
    "            atom_list.append(rest)\n",
    "\n",
    "        atom_info = {}\n",
    "        arguments = {}\n",
    "        list_builtin = []\n",
    "        for atom in atom_list:\n",
    "            if (atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'), None) in ontology:\n",
    "                variable = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'))).split('/')[-1]\n",
    "                atom_info[variable] = variable\n",
    "        head_atoms.append(atom_info)\n",
    "\n",
    "\n",
    "    for item in head_atoms:\n",
    "        key = next(iter(item))  \n",
    "        swrl_rules[key] = body_atoms.pop(0)\n",
    "    \n",
    "    keys_to_delete = [key for key in swrl_rules if key.startswith('Algebric')]\n",
    "    for key in keys_to_delete:\n",
    "        del swrl_rules[key]\n",
    "\n",
    "    composition_rules = {}\n",
    "    for key, value in swrl_rules.items():\n",
    "        new_key = key.split('_', 1)[1]\n",
    "        composition_rules[new_key] = value\n",
    " \n",
    "    data = apply_composition_rules_to_dataset(data, composition_rules)\n",
    "\n",
    "    return data\n",
    "\n",
    "def algebraic(ontology, data):\n",
    "    swrl_rules = {}\n",
    "\n",
    "    body_atoms = []\n",
    "\n",
    "    for s, p, o in ontology.triples((None, URIRef('http://www.w3.org/2003/11/swrl#body'), None)):\n",
    "        atom_list = [o]\n",
    "        while True:\n",
    "            for s, p, o in ontology.triples((atom_list[-1], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first'), None)):\n",
    "                atom_list.append(o)\n",
    "            rest = next(ontology.objects(atom_list[-2], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest')))\n",
    "            if rest == URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#nil'):\n",
    "                break\n",
    "            atom_list.append(rest)\n",
    "\n",
    "        atom_info = {}\n",
    "        arguments = {}\n",
    "        list_builtin = []\n",
    "        variables_connection = {}\n",
    "        atom_info['argument_variables'] = arguments\n",
    "        for atom in atom_list:\n",
    "            if (atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'), None) in ontology:\n",
    "                variable = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'))).split('/')[-1]\n",
    "                argument = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#argument1'))).split('/')[-1]\n",
    "                variables_connection[argument] = variable\n",
    "                atom_info['argument_variables'][variable] = variable\n",
    "            elif (atom, URIRef('http://www.w3.org/2003/11/swrl#builtin'), None) in ontology:\n",
    "                builtin = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#builtin'))).split('/')[-1]\n",
    "                arguments = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#arguments'))).split('/')[-1]\n",
    "                arguments = list(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#arguments')))\n",
    "                for arg in arguments:\n",
    "                    list_items = {}\n",
    "                    list_items[builtin.split('#')[-1]] = []\n",
    "                    for item in ontology.items(arg):\n",
    "                        # If the list item is a resource, print its URI\n",
    "                        if isinstance(item, URIRef):\n",
    "                            if str(item).split('/')[-1] in variables_connection:\n",
    "                                list_items[builtin.split('#')[-1]].append(variables_connection[str(item).split('/')[-1]])\n",
    "                            else:\n",
    "                                list_items[builtin.split('#')[-1]].append(str(item).split('/')[-1])\n",
    "                        # If the list item is a literal, print its value\n",
    "                        elif isinstance(item, Literal):\n",
    "                            list_items[builtin.split('#')[-1]].append(str(item))\n",
    "                    list_builtin.append(list_items)\n",
    "                atom_info['builtins'] = list_builtin\n",
    "        body_atoms.append(atom_info)\n",
    "\n",
    "\n",
    "    head_atoms = []\n",
    "\n",
    "    for s, p, o in ontology.triples((None, URIRef('http://www.w3.org/2003/11/swrl#head'), None)):\n",
    "        atom_list = [o]\n",
    "        while True:\n",
    "            for s, p, o in ontology.triples((atom_list[-1], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#first'), None)):\n",
    "                atom_list.append(o)\n",
    "            rest = next(ontology.objects(atom_list[-2], URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#rest')))\n",
    "            if rest == URIRef('http://www.w3.org/1999/02/22-rdf-syntax-ns#nil'):\n",
    "                break\n",
    "            atom_list.append(rest)\n",
    "\n",
    "        atom_info = {}\n",
    "        arguments = {}\n",
    "        list_builtin = []\n",
    "        for atom in atom_list:\n",
    "            if (atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'), None) in ontology:\n",
    "                variable = next(ontology.objects(atom, URIRef('http://www.w3.org/2003/11/swrl#classPredicate'))).split('/')[-1]\n",
    "                atom_info[variable] = variable\n",
    "        head_atoms.append(atom_info)\n",
    "\n",
    "\n",
    "    for item in head_atoms:\n",
    "        key = next(iter(item))  \n",
    "        swrl_rules[key] = body_atoms.pop(0)\n",
    "    \n",
    "    keys_to_delete = [key for key in swrl_rules if key.startswith('Composition')]\n",
    "    for key in keys_to_delete:\n",
    "        del swrl_rules[key]\n",
    "\n",
    "    algebric_rules = {}\n",
    "    for key, value in swrl_rules.items():\n",
    "        new_key = key.split('_', 1)[1]\n",
    "        algebric_rules[new_key] = value\n",
    " \n",
    "    data = apply_algebric_rules_to_dataset(data, algebric_rules)\n",
    "\n",
    "    return data\n",
    "\n",
    "       \n",
    "def apply_algebric_rules_to_dataset(data, swrl_rules):\n",
    "    for idx, row in data.iterrows():\n",
    "        modified_row = apply_algebric_rules_to_row(row, swrl_rules)\n",
    "\n",
    "        for key, value in modified_row.items():\n",
    "            data.at[idx, key] = value\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_composition_rules_to_dataset(data, composition_rules):\n",
    "    for idx, row in data.iterrows():\n",
    "        modified_row = apply_composition_rules_to_row(row, composition_rules,data)\n",
    "\n",
    "        for key, value in modified_row.items():\n",
    "            data.at[idx, key] = value\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def apply_composition_rules_to_row(row, composition_rules,data):\n",
    "    intermediate_results = {}\n",
    "    \n",
    "    for rule_name, rule in composition_rules.items():\n",
    "        operations = rule['builtins']\n",
    "        final_operation = operations[-1]\n",
    "        \n",
    "        # Perform intermediate operations\n",
    "        for ops in operations[:-1]:\n",
    "            op_type = list(ops.keys())[0]\n",
    "            args = ops[op_type]\n",
    "            arg1, arg2, arg3 = args\n",
    "            if row[arg2] == '' or row[arg3] == '':\n",
    "                intermediate_results[arg1] = None\n",
    "                continue\n",
    "            if op_type == 'divide':\n",
    "                try:\n",
    "                    result = float(row[arg2]) / float(row[arg3])\n",
    "                except ZeroDivisionError:\n",
    "                    result = None\n",
    "                intermediate_results[arg1] = result\n",
    "            elif op_type == 'multiply':\n",
    "                result = float(row[arg2]) * float(row[arg3])\n",
    "                intermediate_results[arg1] = result\n",
    "            elif op_type == 'subtract':\n",
    "                result = float(row[arg2]) - float(row[arg3])\n",
    "                intermediate_results[arg1] = result\n",
    "    \n",
    "        final_op_type = list(final_operation.keys())[0]\n",
    "        final_op_args = final_operation[final_op_type]\n",
    "        \n",
    "        # Perform final operation\n",
    "        if final_op_type == 'subtract':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            result = int(row[arg2]) - int(row[arg3])\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'divide':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            try:\n",
    "                result = int(row[arg2]) / int(row[arg3])\n",
    "            except ZeroDivisionError:\n",
    "                result = None\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'abs':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            result = np.mean(data[arg2].to_list())\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'multiply':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is not None:\n",
    "                if arg3.isdigit():\n",
    "                    result = intermediate_results[arg2] * int(arg3)\n",
    "                else:\n",
    "                    result = intermediate_results[arg2] * row[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is not None:\n",
    "                if arg2.isdigit():\n",
    "                    result = int(arg2) * intermediate_results[arg3]\n",
    "                else:\n",
    "                    result = row[arg2] * intermediate_results[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg2.isdigit():\n",
    "                result = int(arg2) * int(row[arg3])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3.isdigit():\n",
    "                result = int(row[arg2]) * int(arg3)\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            result = int(row[arg2]) * int(row[arg3])\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'greaterThan':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is not None:\n",
    "                if arg3.isdigit():\n",
    "                    result = intermediate_results[arg2] > int(arg3)\n",
    "                else:\n",
    "                    result = intermediate_results[arg2] > int(row[arg3])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is not None:\n",
    "                if arg2.isdigit():\n",
    "                    result = int(arg2) > intermediate_results[arg3]\n",
    "                else:\n",
    "                    result = int(row[arg2]) > intermediate_results[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue            \n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg2.isdigit():\n",
    "                row[rule_name] = int(arg2) > int(row[arg3])\n",
    "                continue\n",
    "            if arg3.isdigit():\n",
    "                row[rule_name] = int(row[arg2]) > int(arg3)\n",
    "                continue\n",
    "            row[rule_name] = int(row[arg2]) > int(row[arg3])\n",
    "            continue\n",
    "        # Add conditions for other final operations if needed\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "def apply_algebric_rules_to_row(row, algebric_rules):\n",
    "    intermediate_results = {}\n",
    "    \n",
    "    for rule_name, rule in algebric_rules.items():\n",
    "        operations = rule['builtins']\n",
    "        final_operation = operations[-1]\n",
    "        \n",
    "        # Perform intermediate operations\n",
    "        for ops in operations[:-1]:\n",
    "            op_type = list(ops.keys())[0]\n",
    "            args = ops[op_type]\n",
    "            arg1, arg2, arg3 = args\n",
    "            if row[arg2] == '' or row[arg3] == '':\n",
    "                intermediate_results[arg1] = None\n",
    "                continue\n",
    "            if op_type == 'divide':\n",
    "                try:\n",
    "                    result = float(row[arg2]) / float(row[arg3])\n",
    "                except ZeroDivisionError:\n",
    "                    result = None\n",
    "                intermediate_results[arg1] = result\n",
    "            elif op_type == 'multiply':\n",
    "                result = float(row[arg2]) * float(row[arg3])\n",
    "                intermediate_results[arg1] = result\n",
    "            elif op_type == 'subtract':\n",
    "                result = float(row[arg2]) - float(row[arg3])\n",
    "                intermediate_results[arg1] = result\n",
    "    \n",
    "        final_op_type = list(final_operation.keys())[0]\n",
    "        final_op_args = final_operation[final_op_type]\n",
    "        \n",
    "        # Perform final operation\n",
    "        if final_op_type == 'add':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in energy_types:\n",
    "                result = 0\n",
    "                for energy in energy_types:\n",
    "                    result = result + int(row[energy])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg2 in renewable_types:\n",
    "                result = 0\n",
    "                for energy in renewable_types:\n",
    "                    result = result + int(row[energy])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "        if final_op_type == 'sin':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            result = haversine(int(row[arg2]),int(row[arg3]),center_baltimore[0],center_baltimore[1])\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'multiply':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is not None:\n",
    "                if arg3.isdigit():\n",
    "                    result = intermediate_results[arg2] * int(arg3)\n",
    "                else:\n",
    "                    result = intermediate_results[arg2] * row[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is not None:\n",
    "                if arg2.isdigit():\n",
    "                    result = int(arg2) * intermediate_results[arg3]\n",
    "                else:\n",
    "                    result = row[arg2] * intermediate_results[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg2.isdigit():\n",
    "                result = int(arg2) * int(row[arg3])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3.isdigit():\n",
    "                result = int(row[arg2]) * int(arg3)\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            result = int(row[arg2]) * int(row[arg3])\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'divide':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is not None:\n",
    "                if arg3.isdigit():\n",
    "                    try:\n",
    "                        result = intermediate_results[arg2] / int(arg3)\n",
    "                    except ZeroDivisionError:\n",
    "                        result = None\n",
    "                    row[rule_name] = result\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        result = intermediate_results[arg2] / int(row[arg3])\n",
    "                    except ZeroDivisionError:\n",
    "                        result = None\n",
    "                    row[rule_name] = result\n",
    "                    continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is not None:\n",
    "                if arg2.isdigit():\n",
    "                    try:\n",
    "                        result = int(arg2) / intermediate_results[arg3]\n",
    "                    except ZeroDivisionError:\n",
    "                        result = None\n",
    "                    row[rule_name] = result\n",
    "                    continue\n",
    "                else:\n",
    "                    try:\n",
    "                        result = int(row[arg2]) / intermediate_results[arg3]\n",
    "                    except ZeroDivisionError:\n",
    "                        result = None\n",
    "                    row[rule_name] = result\n",
    "                    continue\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg2.isdigit():\n",
    "                try:\n",
    "                    result = int(arg2) / int(row[arg3])\n",
    "                except ZeroDivisionError:\n",
    "                    result = None\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3.isdigit():\n",
    "                try:\n",
    "                    result = int(row[arg2]) / int(arg3)\n",
    "                except ZeroDivisionError:\n",
    "                    result = None\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            try:\n",
    "                result = int(row[arg2]) / int(row[arg3])\n",
    "            except ZeroDivisionError:\n",
    "                result = None\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        if final_op_type == 'subtract':\n",
    "            arg1, arg2, arg3 = final_op_args\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is not None:\n",
    "                if arg3.isdigit():\n",
    "                    result = intermediate_results[arg2] - int(arg3)\n",
    "                else:\n",
    "                    result = intermediate_results[arg2] - row[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is not None:\n",
    "                if arg2.isdigit():\n",
    "                    result = int(arg2) - intermediate_results[arg3]\n",
    "                else:\n",
    "                    result = row[arg2] - intermediate_results[arg3]\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg2 in intermediate_results and intermediate_results[arg2] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg3 in intermediate_results and intermediate_results[arg3] is None:\n",
    "                row[rule_name] = None\n",
    "                continue\n",
    "            if arg2.isdigit():\n",
    "                result = int(arg2) - int(row[arg3])\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if arg3.isdigit():\n",
    "                result = int(row[arg2]) - int(arg3)\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            if is_date(row[arg2]):\n",
    "                if isinstance(row[arg2],str):\n",
    "                    date1 = datetime.datetime.strptime(row[arg2], \"%d/%m/%Y\")\n",
    "                    date2 = datetime.datetime.strptime(row[arg3], \"%d/%m/%Y\")\n",
    "                    result = (date2.year - date1.year) * 12 + date2.month - date1.month\n",
    "                    row[rule_name] = result\n",
    "                    continue\n",
    "                value1 = row[arg2]\n",
    "                value2 = row[arg3]\n",
    "                aux1 = value1.strftime('%Y-%m-%d')\n",
    "                aux2 = value2.strftime('%Y-%m-%d')\n",
    "                date1 = datetime.datetime.strptime(aux1, \"%Y-%m-%d\")\n",
    "                date2 = datetime.datetime.strptime(aux2, \"%Y-%m-%d\")\n",
    "                result = (date2.year - date1.year) * 12 + date2.month - date1.month\n",
    "                row[rule_name] = result\n",
    "                continue\n",
    "            result = int(row[arg2]) - int(row[arg3])\n",
    "            row[rule_name] = result\n",
    "            continue\n",
    "        # Add conditions for other final operations if needed\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "def extract_variables_decomp(ontology):\n",
    "    namespace = Namespace(\"http://www.semanticweb.org/rodrigo/ontologies/2024/1/covid/\") #mudar depois para mais geral\n",
    "\n",
    "    vars = {}\n",
    "\n",
    "    # Iterate over the triples to find the DASE_RULE section\n",
    "    for subj, _, obj in ontology.triples((None, namespace[\"DASE_RULE\"], None)):\n",
    "        # Extract the rule from the DASE_RULE section\n",
    "        if obj.startswith(\"Decomposition\"):\n",
    "            # Use regex to extract the variables\n",
    "            match1 = re.findall(r\"___(\\w+)\\([^)]*\\)\", obj)\n",
    "            match2 = re.findall(r\"-> (\\w+)\\(\",obj)\n",
    "            \n",
    "            # Append the extracted variables to the list\n",
    "            if match1 and match2:\n",
    "                vars[match2[0]] = match1[0]\n",
    "    return vars\n",
    "\n",
    "def extract_variables_mapping(ontology):\n",
    "    namespace = Namespace(\"http://www.semanticweb.org/rodrigo/ontologies/2024/1/covid/\") #mudar depois para mais geral\n",
    "\n",
    "    vars = {}\n",
    "\n",
    "    for subj, _, obj in ontology.triples((None, namespace[\"DASE_RULE\"], None)):\n",
    "        # Extract the rule from the DASE_RULE section\n",
    "        if obj.startswith(\"Mapping\"):\n",
    "            # Use regex to extract the variables\n",
    "            match1 = re.findall(r\"___(\\w+)\\([^)]*\\)\", obj)\n",
    "            match2 = re.findall(r\"-> (\\w+)\\(\",obj)\n",
    "            \n",
    "            # Append the extracted variables to the list\n",
    "            if match1 and match2:\n",
    "                vars[match2[0]] = match1[0]\n",
    "    return vars\n",
    "\n",
    "\n",
    "def extract_variables_aggregation(ontology):\n",
    "    namespace = Namespace(\"http://www.semanticweb.org/rodrigo/ontologies/2024/1/covid/\") #mudar depois para mais geral\n",
    "\n",
    "    vars = {}\n",
    "    operations = ['sum', 'avg'] #add more operations like stdev etc\n",
    "\n",
    "    # Iterate over the triples to find the DASE_RULE section\n",
    "    for subj, _, obj in ontology.triples((None, namespace[\"DASE_RULE\"], None)):\n",
    "        # Extract the rule from the DASE_RULE section\n",
    "        if obj.startswith(\"Aggregation\"):\n",
    "            variables = []\n",
    "            # Use regex to extract the variables\n",
    "            match1 = re.findall(r\"___(\\w+)\\([^)]*\\)\", obj)\n",
    "            match2 = re.findall(r'\\^ (\\w+)\\(', obj)\n",
    "            match3 = re.findall(r\"-> (\\w+)\\(\",obj)\n",
    "            \n",
    "            # Append the extracted variables to the list\n",
    "            if match1 and match2 and match3:\n",
    "                variables.append(match1[0])\n",
    "                for n in range(len(match2)):\n",
    "                    variables.append(match2[n])\n",
    "                vars[match3[0]] = variables\n",
    "            for n in range(len(vars[match3[0]])):\n",
    "                if vars[match3[0]][n] in operations:\n",
    "                    tmp = vars[match3[0]][n]\n",
    "                    aux = vars[match3[0]][0]\n",
    "                    vars[match3[0]].remove(aux)\n",
    "                    vars[match3[0]].remove(tmp)\n",
    "                    vars[match3[0]].insert(0,tmp)\n",
    "                    vars[match3[0]].insert(1,aux)\n",
    "    return vars\n",
    "\n",
    "def is_date(string):\n",
    "    if isinstance(string, float):\n",
    "        return False\n",
    "    formats = [\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\"] #Add more formats if needed\n",
    "    if isinstance(string, str):\n",
    "        for fmt in formats:\n",
    "            try:\n",
    "                datetime.datetime.strptime(string, fmt)\n",
    "                return True\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return False\n",
    "    date = string.strftime('%Y-%m-%d')\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            datetime.datetime.strptime(date, fmt)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def apply_decomp_rules(value, new_variable):\n",
    "    result = []\n",
    "    if is_date(value):\n",
    "        if isinstance(value, str):\n",
    "            if(new_variable == 'current_day'):\n",
    "                    date = datetime.datetime.strptime(value, \"%d/%m/%Y\")\n",
    "                    day = date.day\n",
    "                    result.append(new_variable)\n",
    "                    result.append(day)\n",
    "                    return result\n",
    "            elif(new_variable == 'current_month'):\n",
    "                date = datetime.datetime.strptime(value, \"%d/%m/%Y\")\n",
    "                month = date.month\n",
    "                result.append(new_variable)\n",
    "                result.append(month)\n",
    "                return result\n",
    "            elif(new_variable == 'current_year'):\n",
    "                date = datetime.datetime.strptime(value, \"%d/%m/%Y\")\n",
    "                year = date.year\n",
    "                result.append(new_variable)\n",
    "                result.append(year)\n",
    "                return result\n",
    "        date = value.strftime('%Y-%m-%d')\n",
    "        if(new_variable == 'current_day'):\n",
    "            date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            day = date.day\n",
    "            result.append(new_variable)\n",
    "            result.append(day)\n",
    "            return result\n",
    "        elif(new_variable == 'current_month'):\n",
    "            date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            month = date.month\n",
    "            result.append(new_variable)\n",
    "            result.append(month)\n",
    "            return result\n",
    "        elif(new_variable == 'current_year'):\n",
    "            date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            year = date.year\n",
    "            result.append(new_variable)\n",
    "            result.append(year)\n",
    "            return result\n",
    "        elif(new_variable == 'day_period'):\n",
    "            res = getDayPeriod(value,day_periods)\n",
    "            result.append(new_variable)\n",
    "            result.append(res)\n",
    "            return result\n",
    "        elif(new_variable == 'energy_price'):\n",
    "            res = getDayPeriod(value, energy_prices)\n",
    "            result.append(new_variable)\n",
    "            result.append(res)\n",
    "            return result\n",
    "    else:\n",
    "        if(new_variable == 'PM10_safe'):\n",
    "            res = value >= 150\n",
    "            result.append(new_variable)\n",
    "            result.append(res)\n",
    "            return result\n",
    "        elif(new_variable == 'SO2_safe'):\n",
    "            res = value >= 0.14\n",
    "            result.append(new_variable)\n",
    "            result.append(res)\n",
    "            return result\n",
    "    #add more cases if needed\n",
    "\n",
    "def apply_mapping_rules(value, new_variable):\n",
    "    result = []\n",
    "    if is_date(value):\n",
    "        if isinstance(value, str):\n",
    "           if(new_variable == 'Season'):\n",
    "            date = datetime.datetime.strptime(value, \"%d/%m/%Y\")\n",
    "            month = date.month\n",
    "            day = date.day\n",
    "            if (month == 3 and day >= 20) or (month == 4 or month == 5) or (month == 6 and day < 21):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Spring\")\n",
    "                return result\n",
    "            elif (month == 6 and day >= 21) or (month == 7 or month == 8) or (month == 9 and day < 23):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Summer\")\n",
    "                return result\n",
    "            elif (month == 9 and day >= 23) or (month == 10 or month == 11) or (month == 12 and day < 21):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Autumn\")\n",
    "                return result\n",
    "            else:\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Winter\")\n",
    "                return result \n",
    "        date = value.strftime('%Y-%m-%d')\n",
    "        if(new_variable == 'Season'):\n",
    "            date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "            month = date.month\n",
    "            day = date.day\n",
    "            if (month == 3 and day >= 20) or (month == 4 or month == 5) or (month == 6 and day < 21):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Spring\")\n",
    "                return result\n",
    "            elif (month == 6 and day >= 21) or (month == 7 or month == 8) or (month == 9 and day < 23):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Summer\")\n",
    "                return result\n",
    "            elif (month == 9 and day >= 23) or (month == 10 or month == 11) or (month == 12 and day < 21):\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Autumn\")\n",
    "                return result\n",
    "            else:\n",
    "                result.append(new_variable)\n",
    "                result.append(\"Winter\")\n",
    "                return result\n",
    "        #add more cases if needed\n",
    "    #add more cases if needed\n",
    "        \n",
    "    \n",
    "def get_operation(code,*values):\n",
    "    if code == '+':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values)]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == 'positive_sum':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values) if values[i][1] >= 0]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == 'negative_sum':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values) if values[i][1] <= 0]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == '-':\n",
    "        return values[0][1] - values[1][1]\n",
    "    elif code == '*':\n",
    "        prod_values = [values[i][1] for i,x in enumerate(values)]\n",
    "        return np.prod(prod_values)\n",
    "    elif code == '/':\n",
    "        return round(values[0][1] / values[1][1],2)\n",
    "    elif code == '>=':\n",
    "        return values[0][1] >= values[1][1]\n",
    "    elif code == 'datediff':\n",
    "        return relativedelta(values[0][1],values[1][1])\n",
    "    elif code == 'years':\n",
    "        return values[0][1].years\n",
    "    elif code == 'months':\n",
    "        return values[0][1].years * 12 + values[0][1].months\n",
    "    elif code == 'getHour':\n",
    "        return values[0][1].hour\n",
    "    elif code == 'getDay':\n",
    "        return values[0][1].day\n",
    "    elif code == 'getMonth':\n",
    "        return values[0][1].month\n",
    "    elif code == 'getYear':\n",
    "        return values[0][1].year\n",
    "    elif code == 'getWeekday':\n",
    "        return values[0][1].dayofweek\n",
    "    elif code == 'getSeason':\n",
    "        return getSeason(values[0][1],seasons)\n",
    "    elif code == 'getDayPeriod':\n",
    "        return getDayPeriod(values[0][1],day_periods)\n",
    "    elif code == 'getHoliday':\n",
    "        return generateHoliday(values[0][1],values[1][1],holidays)\n",
    "    elif code == 'getEnergyPrice':\n",
    "        return getDayPeriod(values[0][1],energy_prices)\n",
    "    elif code == 'divide_by_30':\n",
    "        return values[0][1] / 30\n",
    "    elif code == 'getAverage':\n",
    "        return generateAverage(values[0][0])\n",
    "    elif code == 'getMax':\n",
    "        return generateMax(values[0][0])\n",
    "    elif code == 'getMin':\n",
    "        return generateMin(values[0][0])\n",
    "    elif code == 'getStd':\n",
    "        return generateStd(values[0][0])\n",
    "    elif code == 'getMedian':\n",
    "        return generateMedian(values[0][0])\n",
    "    elif code == 'generateAvg_2weeks':\n",
    "        return generateAverage2Weeks(values[0][1],values[1][1])\n",
    "    elif code == 'generateAvg_2w_100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'generateSum_2weeks':\n",
    "        return generateCumulative2Weeks(values[0][1],values[1][1])\n",
    "    elif code == 'generateHighRisk_2weeks':\n",
    "        return generateHighRisk(values[0][1],values[1][1],14)\n",
    "    elif code == 'getLastYearTemp':\n",
    "        return generateLastYearTemp(values[0][1],12)\n",
    "    elif code == 'generateSum_2w_100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'generateEstadio_8ed':\n",
    "        return generateFromTable(values[0][1],values[1][1],estadio)\n",
    "    elif code == 'generateN_8ed':\n",
    "        return generateN8ed(values[0][1])\n",
    "    elif code == 'getDistanceBaltimore':\n",
    "        return haversine(values[0][1],values[1][1],center_baltimore[0],center_baltimore[1])\n",
    "    elif code == 'getCrimeType':\n",
    "        return [x[0] for x in crime_type if x[1] == values[0][1]][0]\n",
    "    elif code == 'getWeapon':\n",
    "        return [x[0] for x in weapon_type if x[1] == values[0][1]][0]\n",
    "    elif code == 'getCases100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'getCurrentRisk':\n",
    "        return values[0][1] > 120\n",
    "    elif code == 'getAverageDiffPos':\n",
    "        return values[0][1] >= 0\n",
    "    elif code == 'generatePM25_safe':\n",
    "        return values[0][1] >= 35\n",
    "    elif code == 'generatePM10_safe':\n",
    "        return values[0][1] >= 150\n",
    "    elif code == 'generateSO2_safe':\n",
    "        return values[0][1] >= 0.14\n",
    "    else:\n",
    "        return lambda *x : x\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "      R = 6372.8\n",
    "      dLat = radians(lat2 - lat1)\n",
    "      dLon = radians(lon2 - lon1)\n",
    "      lat1 = radians(lat1)\n",
    "      lat2 = radians(lat2)\n",
    "      a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "      c = 2*asin(sqrt(a))\n",
    "      return R * c\n",
    "\n",
    "def generateFromTable(t,n,table):\n",
    "    return table[t][n]\n",
    "\n",
    "def generateN8ed(gg_p):\n",
    "    if gg_p == 0:\n",
    "        return 1\n",
    "    elif gg_p >= 1 and gg_p <= 2:\n",
    "        return 2\n",
    "    elif gg_p >= 3 and gg_p <= 6:\n",
    "        return 3\n",
    "    elif gg_p >= 7 and gg_p <= 15:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def generateAverage(column):\n",
    "    return np.mean(temp_data[column].to_list())\n",
    "def generateMax(column):\n",
    "    return np.max(temp_data[column].to_list())\n",
    "def generateMin(column):\n",
    "    return np.min(temp_data[column].to_list())\n",
    "def generateStd(column):\n",
    "    return np.std(temp_data[column].to_list())\n",
    "def generateMedian(column):\n",
    "    return np.median(temp_data[column].to_list())\n",
    "\n",
    "def getSeason(date,season_dict):\n",
    "    date = date.replace(year=2000)\n",
    "    return next(season for season, (start, end) in season_dict\n",
    "                if start <= date <= end)\n",
    "def getDayPeriod(date,season_dict):\n",
    "    date = date.replace(year=2000,month=1,day=1)\n",
    "    return next(season for season, (start, end) in season_dict\n",
    "                if start <= date <= end)\n",
    "\n",
    "def generateAverage2Weeks(date,cases):\n",
    "    lastCases = temp_data['cases'].to_list()\n",
    "    return np.mean(lastCases)\n",
    "\n",
    "def generateHighRisk(date,sum_2weeks,offset):\n",
    "    high_risk_day = date + datetime.timedelta(days=offset)\n",
    "    high_risk_cumulative = temp_data[temp_data['current_date'] == high_risk_day]\n",
    "    if len(high_risk_cumulative) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return high_risk_cumulative.iloc[0]['sum_2w_100k'] >= 120.0\n",
    "\n",
    "def generateLastYearTemp(date,offset):\n",
    "    last_year_day = date - relativedelta(months=offset)\n",
    "    last_year_record = temp_data[temp_data['current_date'] == last_year_day]\n",
    "    if len(last_year_record) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return last_year_record.iloc[0]['temperature']\n",
    "\n",
    "def generateCumulative2Weeks(date,cases):\n",
    "    lastCases = temp_data['cases'].to_list()\n",
    "    return np.sum(lastCases)\n",
    "\n",
    "def generateHoliday(date,country,holiday_data):\n",
    "    return holiday_data.loc[(holiday_data['date'] == date) & (holiday_data['country'] == country)].any().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv\n",
      "0.18906160000005912\n",
      "Data Preparation 1 Time\n",
      "0.05146280000008119\n"
     ]
    }
   ],
   "source": [
    "print('read csv')\n",
    "print(read_time)\n",
    "\n",
    "# # First data preparation - MVs\n",
    "start_time = timeit.default_timer()\n",
    "data_observed = data_preparation(data,options,target)\n",
    "print('Data Preparation 1 Time')\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "# # Add automatic variables to ER Model before generation\n",
    "start_time = timeit.default_timer()\n",
    "#model = generateAutoVariables(data_observed,options,template,model)\n",
    "#print(timeit.default_timer() - start_time)\n",
    "\n",
    "#data_observed = dankfe_4(ontology,data_observed)\n",
    "#print('DANKFE4 time')\n",
    "#print(timeit.default_timer() - start_time)\n",
    "\n",
    "data_observed = dankfe_5(ontology,data_observed)\n",
    "\n",
    "# Scaling and Balancing\n",
    "#start_time = timeit.default_timer()\n",
    "#data_observed_train, data_observed_test = data_preparation_2(data_observed,options,target)\n",
    "#print('Data Preparation 2 Time')\n",
    "#print(timeit.default_timer() - start_time)\n",
    "\n",
    "#print('Saving csv Time')\n",
    "#start_time = timeit.default_timer()\n",
    "#data_observed_train.to_csv(f'{data_folder}{base_name}_dankfe4_train.csv',index=False)\n",
    "#data_observed_test.to_csv(f'{data_folder}{base_name}_dankfe4_test.csv',index=False)\n",
    "#print(timeit.default_timer() - start_time)\n",
    "#sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
