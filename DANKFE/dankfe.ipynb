{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/aq/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'utils')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ds_charts as ds\n",
    "from ds_charts import HEIGHT\n",
    "import numpy as np\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "import seaborn as sns\n",
    "import json\n",
    "import jstyleson\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler ,LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# sys.stdout = open('Results/dankfe_1_timings.txt', 'a')\n",
    "\n",
    "\n",
    "base_name = \"aq\"\n",
    "data_folder = f'data/{base_name}/'\n",
    "model_folder = f'data/{base_name}/'\n",
    "print(data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "start_time = timeit.default_timer()\n",
    "if base_name == \"covid\":\n",
    "    data = pd.read_csv(f'{data_folder}{base_name}_base.csv',parse_dates=['current_date','first_date'], infer_datetime_format=True)\n",
    "else:\n",
    "    data = pd.read_csv(f'{data_folder}{base_name}_base.csv',parse_dates=['current_date'], infer_datetime_format=True)\n",
    "# print('read csv')\n",
    "read_time = timeit.default_timer() - start_time\n",
    "\n",
    "# Getting ER Model of case study data\n",
    "f = open(f'{model_folder}{base_name}_model.json')\n",
    "model = jstyleson.load(f)\n",
    "\n",
    "# Getting options JSON\n",
    "f = open(f'{model_folder}options.json')\n",
    "options = jstyleson.load(f)\n",
    "\n",
    "# Getting variable template JSON\n",
    "f = open(f'utils/var_template.json')\n",
    "template = jstyleson.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ALARM'\n",
    "\n",
    "# target_count = data[target].value_counts()\n",
    "# positive_class = target_count.idxmin()\n",
    "# negative_class = target_count.idxmax()\n",
    "# print('Minority class=', positive_class, ':', target_count[positive_class])\n",
    "# print('Majority class=', negative_class, ':', target_count[negative_class])\n",
    "# print('Proportion:', round(target_count[positive_class] / target_count[negative_class], 2), ': 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_name_list = [entity['name'] for entity in model['entities']]\n",
    "# # Raise Error if entities do not match columns:\n",
    "# if collections.Counter(entity_name_list) != collections.Counter(data.columns.to_list()):\n",
    "#     raise ValueError(\"Entities and Columns do not have the same size.\")\n",
    "\n",
    "# Remove all non-observed columns\n",
    "not_observed = [entity['name'] for entity in model['entities'] if not entity['observed']]\n",
    "\n",
    "# Get names of columns to create\n",
    "created_vars = [relation['output'] for relation in model['relations']]\n",
    "\n",
    "# Convert date entities to timestamp\n",
    "date_columns = [entity['name'] for entity in model['entities'] if entity['type'] == 'datetime']\n",
    "# for col in date_columns:\n",
    "#     data[col] = pd.to_datetime(data[col], format = \"%Y-%m-%d\")\n",
    "# data['current_date'] = pd.to_datetime(data['current_date'], format = \"%Y-%m-%d %H:%M:%S\")\n",
    "# data['current_date'] = pd.to_datetime(data['current_date'], format = \"%d/%m/%Y\")\n",
    "# data['first_date'] = pd.to_datetime(data['first_date'], format = \"%d/%m/%Y\")\n",
    "data_observed = data.drop(columns = not_observed)\n",
    "# data_observed = data_observed.sample(frac = 0.1, random_state = 0).reset_index(drop = True)\n",
    "# data_observed = data_observed[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.dtypes)\n",
    "# ds.get_variable_types(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "\n",
    "def data_preparation(dataset,options,target):\n",
    "    if dataset[target].isna().sum() != 0:\n",
    "        # print(\"Target MVs: dropped rows\")\n",
    "        dataset = dataset.dropna(subset=[target]).reset_index(drop = True)\n",
    "    dataset = encodeLabels(dataset)\n",
    "    if options['checkMissingValues'] != 'none':\n",
    "        dataset = checkMissingValues(dataset,options['checkMissingValues'])\n",
    "    return dataset\n",
    "    # generate Variables\n",
    "    # if options['checkScaling'] != 'none':\n",
    "    #     dataset = checkScaling(dataset,options['checkScaling'])\n",
    "    # # divide into train and test\n",
    "    # data_train, data_test = train_test_split(dataset,train_size=0.7,test_size=0.3,stratify=dataset[target],random_state=1)\n",
    "    # if options['checkBalancing']:\n",
    "    #     data_train = checkBalancing(data_train)\n",
    "    # return data_train,data_test\n",
    "\n",
    "def data_preparation_2(dataset,options,target):\n",
    "    if options['checkScaling'] != 'none':\n",
    "        dataset = checkScaling(dataset,options['checkScaling'])\n",
    "    data_train, data_test = train_test_split(dataset,train_size=0.7,test_size=0.3,stratify=dataset[target],random_state=1)\n",
    "    if options['checkBalancing']:\n",
    "        data_train = checkBalancing(data_train)\n",
    "    data_train = data_train.reset_index(drop=True)\n",
    "    data_test = data_test.reset_index(drop=True)\n",
    "    return data_train,data_test\n",
    "\n",
    "def encodeLabels(dataset):\n",
    "    le = LabelEncoder()\n",
    "    bool_df = dataset.select_dtypes(include='bool')\n",
    "    if len(bool_df.columns) != 0:\n",
    "        for col in bool_df:\n",
    "            dataset[col] = le.fit_transform(dataset[col])\n",
    "    return dataset\n",
    "\n",
    "def checkScaling(dataset,method):\n",
    "    # print(f\"Checking scaling: method {method}\")\n",
    "    numeric_vars, symbolic_vars, binary_vars, date_vars = ds.get_variable_types(dataset).values()\n",
    "    \n",
    "    if method == 'zscore':\n",
    "        transf = StandardScaler(with_mean=True, with_std=True, copy=True).fit(dataset[numeric_vars])\n",
    "    if method == 'minmax':\n",
    "        transf = MinMaxScaler(feature_range=(0,1), copy=True).fit(dataset[numeric_vars])\n",
    "    tmp = pd.DataFrame(transf.transform(dataset[numeric_vars]), index = dataset.index, columns = numeric_vars)\n",
    "    data_scaled = pd.concat([tmp,dataset[symbolic_vars],dataset[binary_vars],dataset[date_vars]], axis = 1)\n",
    "    return data_scaled\n",
    "\n",
    "def checkBalancing(dataset):\n",
    "    target_count = data[target].value_counts()\n",
    "    proportion = target_count[target_count.idxmin()] / target_count[target_count.idxmax()]\n",
    "    data_bal = dataset.copy(deep=True)\n",
    "    df_min = data_bal[data_bal[target] == target_count.idxmin()]\n",
    "    df_max = data_bal[data_bal[target] == target_count.idxmax()]\n",
    "    if proportion <= 0.66:\n",
    "        if target_count[target_count.idxmin()] >= 25000 and target_count[target_count.idxmax()] >= 25000:\n",
    "            print(\"Checking balancing: Undersampling both classes - both over 25000\")\n",
    "            df_min = df_min.sample(n=25000, replace = False, random_state = 1)\n",
    "            df_max = df_max.sample(n=25000, replace = False, random_state = 1)\n",
    "        elif target_count[target_count.idxmin()] < 25000 and target_count[target_count.idxmax()] >= 25000:\n",
    "            print(\"Checking balancing: Oversampling min class and undersampling max class - one over 25000\")\n",
    "            df_min = df_min.sample(n=25000, replace = True, random_state = 1)\n",
    "            df_max = df_max.sample(n=25000, replace = False, random_state = 1)\n",
    "        else:\n",
    "            print(\"Checking balancing: Oversampling min class - none over 25000\")\n",
    "            df_min = df_min.sample(n=len(df_max), replace = True, random_state = 1)\n",
    "        data_balanced = pd.concat([df_min,df_max],axis = 0)\n",
    "        return data_balanced\n",
    "    else:\n",
    "        print(\"Checking balancing: No balancing required.\")\n",
    "        return dataset\n",
    "\n",
    "def checkMissingValues(dataset,method):\n",
    "    # print(f\"Checking MVs: method {method}\")\n",
    "    numeric_vars, binary_vars, date_vars, symbolic_vars = ds.get_variable_types(dataset).values()\n",
    "    if method == 'auto':\n",
    "        tmp_nr, tmp_sb, tmp_bool = None, None, None\n",
    "        if len(numeric_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='median', missing_values=np.nan, copy=True)\n",
    "            tmp_nr = pd.DataFrame(imp.fit_transform(dataset[numeric_vars]), index =dataset.index ,columns=numeric_vars)\n",
    "        if len(symbolic_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='most_frequent', missing_values=np.nan, copy=True)\n",
    "            tmp_sb = pd.DataFrame(imp.fit_transform(dataset[symbolic_vars]), index=dataset.index ,columns=symbolic_vars)\n",
    "        if len(binary_vars) > 0:\n",
    "            imp = SimpleImputer(strategy='most_frequent', missing_values=np.nan, copy=True)\n",
    "            tmp_bool = pd.DataFrame(imp.fit_transform(dataset[binary_vars]), index=dataset.index ,columns=binary_vars)\n",
    "    data_mv = pd.concat([tmp_nr, tmp_sb, tmp_bool,dataset[date_vars]], axis=1)\n",
    "    return data_mv\n",
    "\n",
    "# data_prepared_train, data_prepared_test = data_preparation(data,options,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic variable generation\n",
    "\n",
    "def generateAutoVariables(dataset,options,template,model):\n",
    "    if options['generateDates']:\n",
    "        model = generateDates(dataset,template['dates'])\n",
    "    if len(options['generateFiveSummary']) != 0:\n",
    "        model = generateFiveSummary(dataset,template['five_summary'], options['groupby'], options['generateFiveSummary'])\n",
    "    return model\n",
    "\n",
    "def generateDates(dataset,template):\n",
    "    date_vars = ds.get_variable_types(dataset)['Date']\n",
    "    new_datevars = []\n",
    "    template_tmp = copy.deepcopy(template)\n",
    "    for date_col in date_vars:\n",
    "        template_tmp = copy.deepcopy(template)\n",
    "        for new_var in template_tmp:\n",
    "            new_var['output'] = f\"{date_col}_{new_var['output']}\"\n",
    "            new_var['inputs'].append(date_col)\n",
    "            new_datevars.append(new_var)\n",
    "    # print(f\"Adding {len(new_datevars)} new variables\")\n",
    "    model['relations'] =  model['relations'] + new_datevars\n",
    "    return model\n",
    "    \n",
    "def generateFiveSummary(dataset,template,groupby,num_vars):\n",
    "    # num_vars = ds.get_variable_types(dataset)['Numeric']\n",
    "    new_numvars = []\n",
    "    template_tmp = copy.deepcopy(template)\n",
    "    if len(groupby) != 0:\n",
    "        for group in groupby:\n",
    "            for num_col in num_vars:\n",
    "                template_tmp = copy.deepcopy(template)\n",
    "                for new_var in template_tmp:\n",
    "                    new_var['output'] = f\"{num_col}_{new_var['output']}_per_{group}\"\n",
    "                    new_var['inputs'].append(num_col)\n",
    "                    new_var['groupby'] = group\n",
    "                    new_numvars.append(new_var)\n",
    "    # print(f\"Adding {len(new_numvars)} new variables\")\n",
    "    model['relations'] = model['relations'] + new_numvars\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Generation\n",
    "holidays = pd.read_csv('holidays.csv')\n",
    "holidays['date'] =  pd.to_datetime(holidays['date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "day_periods = [(1, (datetime.datetime(2000,1,1,0,0,0),  datetime.datetime(2000,1,1,2,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,3,0,0),  datetime.datetime(2000,1,1,5,59,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,6,0,0),  datetime.datetime(2000,1,1,8,59,59))),\n",
    "           (4, (datetime.datetime(2000,1,1,9,0,0),  datetime.datetime(2000,1,1,11,59,59))),\n",
    "           (5, (datetime.datetime(2000,1,1,12,0,0),  datetime.datetime(2000,1,1,14,59,59))),\n",
    "           (6, (datetime.datetime(2000,1,1,15,0,0),  datetime.datetime(2000,1,1,17,59,59))),\n",
    "           (7, (datetime.datetime(2000,1,1,18,0,0),  datetime.datetime(2000,1,1,20,59,59))),\n",
    "           (8, (datetime.datetime(2000,1,1,21,0,0),  datetime.datetime(2000,1,1,23,59,59)))]\n",
    "\n",
    "energy_prices = [(1, (datetime.datetime(2000,1,1,22,0,0),  datetime.datetime(2000,1,1,23,59,59))),\n",
    "           (1, (datetime.datetime(2000,1,1,0,0,0),  datetime.datetime(2000,1,1,7,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,8,0,0),  datetime.datetime(2000,1,1,8,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,10,30,0),  datetime.datetime(2000,1,1,17,59,59))),\n",
    "           (2, (datetime.datetime(2000,1,1,20,30,0),  datetime.datetime(2000,1,1,21,59,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,9,0,0),  datetime.datetime(2000,1,1,10,29,59))),\n",
    "           (3, (datetime.datetime(2000,1,1,18,0,0),  datetime.datetime(2000,1,1,20,29,59)))]\n",
    "\n",
    "seasons = [(1, (datetime.date(2000,  1,  1),  datetime.date(2000,  3, 20))),\n",
    "           (2, (datetime.date(2000,  3, 21),  datetime.date(2000,  6, 20))),\n",
    "           (3, (datetime.date(2000,  6, 21),  datetime.date(2000,  9, 22))),\n",
    "           (4, (datetime.date(2000,  9, 23),  datetime.date(2000, 12, 20))),\n",
    "           (1, (datetime.date(2000, 12, 21),  datetime.date(2000, 12, 31)))]\n",
    "\n",
    "center_baltimore = (39.30746849825375, -76.61560625253648)\n",
    "\n",
    "crime_type = [\n",
    "    (1, 'LARCENY'), (2,'LARCENY FROM AUTO'), (3,'AUTO THEFT'), (11,'ROBBERY - STREET'), (12,'ROBBERY - COMMERCIAL'), (13,'ROBBERY - CARJACKING'),(14,'ROBBERY - RESIDENCE'),(15,'BURGLARY'),(21,'COMMON ASSAULT'),(31,'ASSAULT BY THREAT'),(32,'AGG. ASSAULT'),(41,'ARSON'),(51,'SHOOTING'),(52,'RAPE'),(61,'HOMICIDE')\n",
    "]\n",
    "weapon_type = [\n",
    "    (0, 'NONE'), (1,'HANDS'), (2,'OTHER'), (3,'KNIFE'), (4,'FIREARM')\n",
    "]\n",
    "\n",
    "# Kept data for operations\n",
    "temp_data = None\n",
    "\n",
    "\n",
    "# DANKFE 2\n",
    "def dankfe_2(model,loop_dataset,edit_dataset):\n",
    "    global temp_data\n",
    "    relations_queue = model['relations']\n",
    "\n",
    "    while len(relations_queue) != 0:\n",
    "        current_relation = relations_queue[0]\n",
    "        print(current_relation['output'])\n",
    "        newvar_name = current_relation['output']\n",
    "        inputs = current_relation['inputs']\n",
    "        groupby = current_relation['groupby']\n",
    "        if set(inputs).issubset(loop_dataset.columns):         # if inputs already exist in the dataset\n",
    "            start_time = timeit.default_timer()\n",
    "            # if len(current_relation['constraint']) == 0:\n",
    "            #     constraint = \"index == index\"\n",
    "            # else:\n",
    "            constraint = current_relation['constraint']\n",
    "            if len(current_relation['constraint']) != 0:\n",
    "                constraint = \"row.\" + constraint\n",
    "            else:\n",
    "                constraint = \"True\"\n",
    "            if len(groupby) == 0: #LAMBDA: no row dependence\n",
    "                for index, op in enumerate(current_relation['operations']):\n",
    "                    if index == 0:\n",
    "                        edit_dataset[newvar_name] = edit_dataset.apply(lambda row: get_operation(op,*zip(inputs,row[inputs])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "                    else:\n",
    "                        edit_dataset[newvar_name] = edit_dataset.apply(lambda row: get_operation(op,zip(newvar_name,row[newvar_name])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "            else:\n",
    "                needed_rows = current_relation['needsRows']\n",
    "                for i in range(len(loop_dataset)):\n",
    "                    # counter += 1\n",
    "                    row = loop_dataset.loc[i]  # we pass the list to get a DataFrame instead of Series\n",
    "                    if len(groupby) != 0:\n",
    "                        if needed_rows == 'all':\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]]\n",
    "                        elif needed_rows < 0:\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]].loc[i + needed_rows+1:i]\n",
    "                        else:\n",
    "                            temp_data = loop_dataset[loop_dataset[groupby] == row.loc[groupby]].loc[i:i + needed_rows-1] # get necessary rows to temp_data\n",
    "                    # else:\n",
    "                    #     if needed_rows < 0: \n",
    "                    #         temp_data = dataset.loc[i+ needed_rows:i]\n",
    "                    #     else:    \n",
    "                    #         temp_data = dataset.loc[i:i + needed_rows]\n",
    "                    if pd.eval(constraint, target = row) == False:\n",
    "                        edit_dataset.loc[i,newvar_name] = np.nan\n",
    "                        continue\n",
    "                    else:\n",
    "                        for opIndex, op in enumerate(current_relation['operations']):\n",
    "                            input_values =  [row.loc[inp] for inp in inputs]\n",
    "                            if opIndex == 0:\n",
    "                                edit_dataset.loc[i,newvar_name] = get_operation(op,*zip(inputs,input_values))\n",
    "                            else:\n",
    "                                edit_dataset.loc[i,newvar_name] = get_operation(op,zip(newvar_name,edit_dataset.loc[i,newvar_name]))\n",
    "            relations_queue = relations_queue[1:]\n",
    "            loop_dataset = edit_dataset.copy(deep=True)\n",
    "            print(timeit.default_timer() - start_time)\n",
    "        else:\n",
    "            # send to the bottom of the queue\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "\n",
    "\n",
    "# DANKFE 1\n",
    "def dankfe_1(model,dataset):\n",
    "    relations_queue = model['relations']\n",
    "\n",
    "    while len(relations_queue) != 0:\n",
    "        current_relation = relations_queue[0]\n",
    "        if len(current_relation['groupby']) != 0:\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "            continue\n",
    "        print(current_relation['output'])\n",
    "        newvar_name = current_relation['output']\n",
    "        inputs = current_relation['inputs']\n",
    "        if set(inputs).issubset(dataset.columns):\n",
    "            start_time = timeit.default_timer()\n",
    "            # if inputs already exist in the dataset\n",
    "            constraint = current_relation['constraint']\n",
    "            if len(current_relation['constraint']) != 0:\n",
    "                constraint = \"row.\" + constraint\n",
    "            else:\n",
    "                constraint = \"True\"\n",
    "            for index, op in enumerate(current_relation['operations']):\n",
    "                if index == 0:\n",
    "                    dataset[newvar_name] = dataset.apply(lambda row: get_operation(op,*zip(inputs,row[inputs])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "                else:\n",
    "                    # dataset[newvar_name] = dataset.apply(lambda row: print(row[newvar_name]), axis = 1)\n",
    "                    dataset[newvar_name] = dataset.apply(lambda row: get_operation(op,(newvar_name,row[newvar_name])) if pd.eval(constraint, target = row) else np.nan, axis = 1)\n",
    "            relations_queue = relations_queue[1:]\n",
    "            print(timeit.default_timer() - start_time)\n",
    "        else:\n",
    "            # send to the bottom of the queue\n",
    "            relations_queue.append(relations_queue.pop(relations_queue.index(current_relation)))\n",
    "\n",
    "    \n",
    "def get_operation(code,*values):\n",
    "    if code == '+':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values)]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == 'positive_sum':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values) if values[i][1] >= 0]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == 'negative_sum':\n",
    "        sum_values = [values[i][1] for i,x in enumerate(values) if values[i][1] <= 0]\n",
    "        return np.sum(sum_values)\n",
    "    elif code == '-':\n",
    "        return values[0][1] - values[1][1]\n",
    "    elif code == '*':\n",
    "        prod_values = [values[i][1] for i,x in enumerate(values)]\n",
    "        return np.prod(prod_values)\n",
    "    elif code == '/':\n",
    "        return round(values[0][1] / values[1][1],2)\n",
    "    elif code == '>=':\n",
    "        return values[0][1] >= values[1][1]\n",
    "    elif code == 'datediff':\n",
    "        return relativedelta(values[0][1],values[1][1])\n",
    "    elif code == 'years':\n",
    "        return values[0][1].years\n",
    "    elif code == 'months':\n",
    "        return values[0][1].years * 12 + values[0][1].months\n",
    "    elif code == 'getHour':\n",
    "        return values[0][1].hour\n",
    "    elif code == 'getDay':\n",
    "        return values[0][1].day\n",
    "    elif code == 'getMonth':\n",
    "        return values[0][1].month\n",
    "    elif code == 'getYear':\n",
    "        return values[0][1].year\n",
    "    elif code == 'getWeekday':\n",
    "        return values[0][1].dayofweek\n",
    "    elif code == 'getSeason':\n",
    "        return getSeason(values[0][1],seasons)\n",
    "    elif code == 'getDayPeriod':\n",
    "        return getDayPeriod(values[0][1],day_periods)\n",
    "    elif code == 'getHoliday':\n",
    "        return generateHoliday(values[0][1],values[1][1],holidays)\n",
    "    elif code == 'getEnergyPrice':\n",
    "        return getDayPeriod(values[0][1],energy_prices)\n",
    "    elif code == 'divide_by_30':\n",
    "        return values[0][1] / 30\n",
    "    elif code == 'getAverage':\n",
    "        return generateAverage(values[0][0])\n",
    "    elif code == 'getMax':\n",
    "        return generateMax(values[0][0])\n",
    "    elif code == 'getMin':\n",
    "        return generateMin(values[0][0])\n",
    "    elif code == 'getStd':\n",
    "        return generateStd(values[0][0])\n",
    "    elif code == 'getMedian':\n",
    "        return generateMedian(values[0][0])\n",
    "    elif code == 'generateAvg_2weeks':\n",
    "        return generateAverage2Weeks(values[0][1],values[1][1])\n",
    "    elif code == 'generateAvg_2w_100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'generateSum_2weeks':\n",
    "        return generateCumulative2Weeks(values[0][1],values[1][1])\n",
    "    elif code == 'generateHighRisk_2weeks':\n",
    "        return generateHighRisk(values[0][1],values[1][1],14)\n",
    "    elif code == 'getLastYearTemp':\n",
    "        return generateLastYearTemp(values[0][1],values[1][1],12)\n",
    "    elif code == 'generateSum_2w_100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'generateEstadio_8ed':\n",
    "        return generateFromTable(values[0][1],values[1][1],estadio)\n",
    "    elif code == 'generateN_8ed':\n",
    "        return generateN8ed(values[0][1])\n",
    "    elif code == 'getDistanceBaltimore':\n",
    "        return haversine(values[0][1],values[1][1],center_baltimore[0],center_baltimore[1])\n",
    "    elif code == 'getCrimeType':\n",
    "        return [x[0] for x in crime_type if x[1] == values[0][1]][0]\n",
    "    elif code == 'getWeapon':\n",
    "        return [x[0] for x in weapon_type if x[1] == values[0][1]][0]\n",
    "    elif code == 'getCases100k':\n",
    "        return values[0][1] * 100000 / values[1][1]\n",
    "    elif code == 'getCurrentRisk':\n",
    "        return values[0][1] > 120\n",
    "    elif code == 'getAverageDiffPos':\n",
    "        return values[0][1] >= 0\n",
    "    elif code == 'generatePM25_safe':\n",
    "        return values[0][1] >= 35\n",
    "    elif code == 'generatePM10_safe':\n",
    "        return values[0][1] >= 150\n",
    "    elif code == 'generateSO2_safe':\n",
    "        return values[0][1] >= 0.14\n",
    "    else:\n",
    "        return lambda *x : x\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "      R = 6372.8\n",
    "      dLat = radians(lat2 - lat1)\n",
    "      dLon = radians(lon2 - lon1)\n",
    "      lat1 = radians(lat1)\n",
    "      lat2 = radians(lat2)\n",
    "      a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
    "      c = 2*asin(sqrt(a))\n",
    "      return R * c\n",
    "\n",
    "def generateFromTable(t,n,table):\n",
    "    return table[t][n]\n",
    "\n",
    "def generateN8ed(gg_p):\n",
    "    if gg_p == 0:\n",
    "        return 1\n",
    "    elif gg_p >= 1 and gg_p <= 2:\n",
    "        return 2\n",
    "    elif gg_p >= 3 and gg_p <= 6:\n",
    "        return 3\n",
    "    elif gg_p >= 7 and gg_p <= 15:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "def generateAverage(column):\n",
    "    return np.mean(temp_data[column].to_list())\n",
    "def generateMax(column):\n",
    "    return np.max(temp_data[column].to_list())\n",
    "def generateMin(column):\n",
    "    return np.min(temp_data[column].to_list())\n",
    "def generateStd(column):\n",
    "    return np.std(temp_data[column].to_list())\n",
    "def generateMedian(column):\n",
    "    return np.median(temp_data[column].to_list())\n",
    "\n",
    "def getSeason(date,season_dict):\n",
    "    date = date.replace(year=2000)\n",
    "    return next(season for season, (start, end) in season_dict\n",
    "                if start <= date <= end)\n",
    "def getDayPeriod(date,season_dict):\n",
    "    date = date.replace(year=2000,month=1,day=1)\n",
    "    return next(season for season, (start, end) in season_dict\n",
    "                if start <= date <= end)\n",
    "\n",
    "def generateAverage2Weeks(date,cases):\n",
    "    lastCases = temp_data['cases'].to_list()\n",
    "    return np.mean(lastCases)\n",
    "\n",
    "def generateHighRisk(date,sum_2weeks,offset):\n",
    "    high_risk_day = date + datetime.timedelta(days=offset)\n",
    "    high_risk_cumulative = temp_data[temp_data['current_date'] == high_risk_day]\n",
    "    if len(high_risk_cumulative) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return high_risk_cumulative.iloc[0]['sum_2w_100k'] >= 120.0\n",
    "\n",
    "def generateLastYearTemp(date,temp,offset):\n",
    "    last_year_day = date - relativedelta(months=offset)\n",
    "    last_year_record = temp_data[temp_data['current_date'] == last_year_day]\n",
    "    if len(last_year_record) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return last_year_record.iloc[0]['temperature']\n",
    "\n",
    "def generateCumulative2Weeks(date,cases):\n",
    "    lastCases = temp_data['cases'].to_list()\n",
    "    return np.sum(lastCases)\n",
    "\n",
    "def generateHoliday(date,country,holiday_data):\n",
    "    return holiday_data.loc[(holiday_data['date'] == date) & (holiday_data['country'] == country)].any().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv\n",
      "0.4526356999995187\n",
      "0.5078981000115164\n",
      "0.08322460000636056\n",
      "{'entities': [{'name': 'current_date', 'type': 'datetime', 'description': 'Date of reporting', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'ALARM', 'type': 'bool', 'description': 'Risk of low quality air', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'CO_Mean', 'type': 'float', 'description': 'CO Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'CO_Min', 'type': 'float', 'description': 'CO Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'CO_Max', 'type': 'float', 'description': 'CO Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'CO_Std', 'type': 'float', 'description': 'CO Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'NO2_Mean', 'type': 'float', 'description': 'NO2 Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'NO2_Min', 'type': 'float', 'description': 'NO2 Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'NO2_Max', 'type': 'float', 'description': 'NO2 Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'NO2_Std', 'type': 'float', 'description': 'NO2 Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'O3_Mean', 'type': 'float', 'description': 'O3 Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'O3_Min', 'type': 'float', 'description': 'O3 Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'O3_Max', 'type': 'float', 'description': 'O3 Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'O3_Std', 'type': 'float', 'description': 'O3 Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM2.5_Mean', 'type': 'float', 'description': 'PM2.5 Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM2.5_Min', 'type': 'float', 'description': 'PM2.5 Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM2.5_Max', 'type': 'float', 'description': 'PM2.5 Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM2.5_Std', 'type': 'float', 'description': 'PM2.5 Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM10_Mean', 'type': 'float', 'description': 'PM10 Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM10_Min', 'type': 'float', 'description': 'PM10 Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM10_Max', 'type': 'float', 'description': 'PM10 Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'PM10_Std', 'type': 'float', 'description': 'PM10 Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'SO2_Mean', 'type': 'float', 'description': 'SO2 Mean', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'SO2_Min', 'type': 'float', 'description': 'SO2 Min', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'SO2_Max', 'type': 'float', 'description': 'SO2 Max', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'SO2_Std', 'type': 'float', 'description': 'SO2 Std', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'Distance', 'type': 'float', 'description': 'Distance to province center', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'Distance_Prov', 'type': 'float', 'description': 'Distance from province center to capital', 'order': 0, 'observed': True, 'constraints': []}, {'name': 'GbCity', 'type': 'int', 'description': 'City Code', 'order': 0, 'observed': True, 'constraints': []}], 'relations': [{'name': 'PM25_safe', 'type': 'bool', 'operations': ['generatePM25_safe'], 'inputs': ['PM2.5_Mean'], 'output': 'PM2.5_safe', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'PM10_safe', 'type': 'bool', 'operations': ['generatePM10_safe'], 'inputs': ['PM10_Mean'], 'output': 'PM10_safe', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'SO2_safe', 'type': 'bool', 'operations': ['generateSO2_safe'], 'inputs': ['SO2_Mean'], 'output': 'SO2_safe', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'PP_Mean', 'type': 'float', 'operations': ['+'], 'inputs': ['PM2.5_Mean', 'PM10_Mean'], 'output': 'PP_Mean', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'day', 'type': 'int', 'operations': ['getDay'], 'inputs': ['current_date'], 'output': 'current_date_day', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'month', 'type': 'int', 'operations': ['getMonth'], 'inputs': ['current_date'], 'output': 'current_date_month', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'year', 'type': 'int', 'operations': ['getYear'], 'inputs': ['current_date'], 'output': 'current_date_year', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'season', 'type': 'str', 'operations': ['getSeason'], 'inputs': ['current_date'], 'output': 'current_date_season', 'groupby': '', 'needsRows': 0, 'constraint': ''}, {'name': 'weekday', 'type': 'int', 'operations': ['getWeekday'], 'inputs': ['current_date'], 'output': 'current_date_weekday', 'groupby': '', 'needsRows': 0, 'constraint': ''}]}\n"
     ]
    }
   ],
   "source": [
    "print('read csv')\n",
    "print(read_time)\n",
    "\n",
    "# # First data preparation - MVs\n",
    "start_time = timeit.default_timer()\n",
    "data_observed = data_preparation(data_observed,options,target)\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "# # Add automatic variables to ER Model before generation\n",
    "start_time = timeit.default_timer()\n",
    "model = generateAutoVariables(data_observed,options,template,model)\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# # DANKFE 2 or 3\n",
    "data_loop = data_observed.copy(deep=True)\n",
    "dankfe_2(model,data_loop,data_observed)\n",
    "\n",
    "# # DANKFE 1 (comment paragraph above if running DANKFE-1)\n",
    "# dankfe_1(model,data_observed)\n",
    "\n",
    "# Scaling and Balancing\n",
    "start_time = timeit.default_timer()\n",
    "data_observed_train, data_observed_test = data_preparation_2(data_observed,options,target)\n",
    "print(timeit.default_timer() - start_time)\n",
    "\n",
    "print('save csv')\n",
    "start_time = timeit.default_timer()\n",
    "data_observed_train.to_csv(f'{data_folder}{base_name}_gen_train.csv',index=False)\n",
    "data_observed_test.to_csv(f'{data_folder}{base_name}_gen_test.csv',index=False)\n",
    "# data_observed.to_csv(f'{data_folder}{base_name}_lambda.csv',index= False)\n",
    "print(timeit.default_timer() - start_time)\n",
    "# sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
